The main task of the entire project will be the following:

- Each row in the dataset is a document
- For each document, generate a set of entities using NER (Named-Entity-Recognition)
- Using these entities (generated from the 'Subtitles'-column), and the text itself, use two different schemes to generate topics
    - One scheme will be using BERT in order to generate topics through ML-methods
    - The other scheme, will be generating topics through LSA (Latent Semantic Analysis)
- Given the two different versions of the topics, use the topics in combination with all other attributes available in order to generate
    different groupings of the articles (where topics are main categories, and we further generate sub-categories)
- The overall finish product will be the grouping of every article into some category/sub-cagetory based on the article's attributes and information
- We will finally end up with a categorization of every article based on article content, entities, topics, etc.

Evaluation:
- We can evaluate the results by using known methods from information retrieval theory (tf-idf, cosine similarity, inverse variance)
- Finally, a last evaluation can be the human (translation and) interpretation of the resulting categorizations
    - These categories can be interpreted by us, and commented upon in order to give a human response to whether or not the categorization somewhat make sense
- Other evaluation techniques can also be included